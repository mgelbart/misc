{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CPSC 330 Lecture 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_rows', 9)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda install python-graphviz\n",
    "# pip install python-graphviz\n",
    "import graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda install scikit-learn\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install git+git://github.com/mgelbart/plot-classifier.git\n",
    "from plot_classifier import plot_classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture outline\n",
    "- Introduction to supervized learning (10 mins)\n",
    "- Tabular data (5 mins)\n",
    "- Training a decision tree using scikit-learn (20 mins)\n",
    "- Decision tree splitting rules (5 mins)\n",
    "- Break (5 mins)\n",
    "- ML model parameters and hyperparameters (10 mins)\n",
    "- True/False questions (25 min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to supervised learning (10 mins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An example of supervised learning\n",
    "- In supervised learning, we have a set of features, $X$, with associated targets, $y$\n",
    "- We wish to find a model function that relates $X$ to $y$\n",
    "- Then use that model function to predict future observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/cities_USA.csv', index_col=0)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blue = df.query('vote == \"blue\"')\n",
    "red  = df.query('vote == \"red\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(blue[\"lon\"], blue[\"lat\"], color=\"blue\", alpha=0.3);\n",
    "plt.scatter(red[\"lon\"], red[\"lat\"], color=\"red\", alpha=0.3);\n",
    "plt.ylabel(\"latitude\");\n",
    "plt.xlabel(\"longitude\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are $X$ and $y$ here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[[\"lon\", \"lat\"]]\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df[[\"vote\"]]\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Note that $X$ is a 2-dimensional array, whereas $y$ is 1-dimensional.\n",
    "- In CPSC 340 we would say $X$ is a matrix and $y$ is a vector, but here's we're avoiding linear algebra. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification vs Regression\n",
    "- Variables can be characterized as quantitative/numeric or qualitative/categorical\n",
    "- **Classification** = prediction of a categorical target (e.g. red vs. blue)\n",
    "- **Regression** = prediction of a quantitative response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='img/regr.png' width=\"750\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification vs Regression questions\n",
    "\n",
    "Which of these are examples of classification? (To answer on Piazza)\n",
    "\n",
    "1. Predicting the price of a house based on features like number of rooms.\n",
    "2. Predicting if a house will sell or not based on features like the price of the house, number of rooms, etc.\n",
    "3. Predicting your grade based on past grades.\n",
    "4. Predicting whether you should bicycle to work tomorrow based on the weather forecast.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unsupervised learning\n",
    "\n",
    "- Later in the course we will discuss _unsupervised learning_ as well.\n",
    "- It is kind of like supervised learning but without the \"y\".\n",
    "- More on supervised vs. unsupervised learning later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tabular data and terminology (5 min)\n",
    "- For ML we typically work with \"tabular data\"\n",
    "- Rows are examples\n",
    "- Columns are features (the last column is typically the target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This dataset contains longtitude and latitude data for 400 cities in the US\n",
    "- Each city is labelled as `red` or `blue` depending on how they voted in the 2012 election.\n",
    "- The cities data was sampled from (http://simplemaps.com/static/demos/resources/us-cities/cities.csv). The election information was collected from Wikipedia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Terminology\n",
    "- You will see a lot of variable terminology in machine learning and statistics\n",
    "- See the MDS terminology resource [here](https://ubc-mds.github.io/resources_pages/terminology/).\n",
    "\n",
    "Of particular note:\n",
    "\n",
    "- **examples** = rows = samples = records = instances (usually denoted by $n$)\n",
    "- **features** = inputs = predictors = explanatory variables = regressors = independent variables = covariates (usually denoted by $d$)\n",
    "- **targets** = outputs = outcomes = response variable = dependent variable = labels (if categorical).\n",
    "- **training** = learning = fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this data set we have 6 examples of 3 variables (2 features, 1 target)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a decision tree using scikit-learn (20 min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using scikit-learn's fit/predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecisionTreeClassifier(max_depth=1)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll pick a few examples at random just for a toy example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/cities_USA.csv', index_col=0).sample(6, random_state=100)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=['vote'])\n",
    "y = df[['vote']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_data = export_graphviz(model)\n",
    "graphviz.Source(export_graphviz(model,\n",
    "                                out_file=None,\n",
    "                                feature_names=X.columns,\n",
    "                                class_names=[\"blue\", \"red\"],\n",
    "                                impurity=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "ax = plt.gca()\n",
    "plot_classifier(X, y, model, ax=ax, ticks=True);\n",
    "plt.ylabel(\"latitude\");\n",
    "plt.xlabel(\"longitude\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we can also predict a brand new (made up) point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "made_up_X = np.array([-85, 30])\n",
    "model.predict(made_up_X[np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a deeper tree now, on the full data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/cities_USA.csv', index_col=0)\n",
    "X = df.drop(columns=['vote'])\n",
    "y = df[['vote']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecisionTreeClassifier(max_depth=1)\n",
    "model.fit(X,y)\n",
    "plt.figure()\n",
    "ax = plt.gca()\n",
    "plot_classifier(X, y, model, ax=ax, ticks=True);\n",
    "plt.ylabel(\"latitude\");\n",
    "plt.xlabel(\"longitude\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecisionTreeClassifier(max_depth=2)\n",
    "model.fit(X,y)\n",
    "plt.figure()\n",
    "ax = plt.gca()\n",
    "plot_classifier(X, y, model, ax=ax, ticks=True);\n",
    "plt.ylabel(\"latitude\");\n",
    "plt.xlabel(\"longitude\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecisionTreeClassifier()\n",
    "model.fit(X,y)\n",
    "plt.figure()\n",
    "ax = plt.gca()\n",
    "plot_classifier(X, y, model, ax=ax, ticks=True);\n",
    "plt.ylabel(\"latitude\");\n",
    "plt.xlabel(\"longitude\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision tree splitting rules (5 mins)\n",
    "\n",
    "- You saw in the video that a tree with only one split is called a \"decision stump\"\n",
    "- How do we decide how to split the data?\n",
    "- Basic idea is to pick a criterion (see [here](https://scikit-learn.org/stable/modules/tree.html#mathematical-formulation)) and then maximize it across possible splits.\n",
    "- It turns out accuracy is not a good metric, so we use some fancier metrics like \"entropy\" or \"gini impurity\".\n",
    "- The basic idea is to try and make each leaf as \"pure\" as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Break (5 mins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  ML model parameters and hyperparameters (10 mins)\n",
    "\n",
    "- When you call `fit`, a bunch of values get set, like the split variables and split thresholds. \n",
    "- These are called **parameters**\n",
    "- But even before calling `fit` on a specific data set, we can set some \"knobs\" that control the learning.\n",
    "- These are called **hyperparameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/cities_USA.csv', index_col=0)\n",
    "X = df.drop(columns=['vote'])\n",
    "y = df[['vote']]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In scikit-learn, hyperparameters are set in the constructor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecisionTreeClassifier(max_depth=3) \n",
    "model.fit(X, y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, `max_depth` is a hyperparameter. There are many, many more! See [here](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_data = export_graphviz(model)\n",
    "graphviz.Source(export_graphviz(model,\n",
    "                                out_file=None,\n",
    "                                feature_names=X.columns,\n",
    "                                class_names=[\"red\", \"blue\"],\n",
    "                                impurity=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To summarize:\n",
    "\n",
    "- **parameters** are automatically learned by the algorithm during training\n",
    "- **hyperparameters** are specified based on:\n",
    "    - expert knowledge\n",
    "    - heuristics, or \n",
    "    - systematic/automated optimization (more on that later on)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preview of next class...\n",
    "\n",
    "- Why not just use a very deep decision tree for every supervised learning problem and get super high accuracy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## True/False questions (25 min)\n",
    "\n",
    "Which of these are true about decision trees?\n",
    "\n",
    "1. Decision trees are typically binary trees (2 children per node).\n",
    "2. Typically, the features that we split on at each node are chosen by a human.\n",
    "3. A decision stump is a decision tree with depth $\\leq 3$.\n",
    "5. The same feature can be split on multiple times in a tree with depth > 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each of the following, answer with `fit` or `predict`:\n",
    "\n",
    "1. At least for decision trees, this is where most of the hard work is done.\n",
    "2. Only takes `X` as an argument.\n",
    "3. In scikit-learn, we can ignore its output.\n",
    "4. Is called first (before the other one)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
